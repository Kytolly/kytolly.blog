<!DOCTYPE html>
<html>
<meta charset="utf-8" />
  <!-- 标签页 -->
  <!-- 标签页 -->
<title>
    IN-CONTEXT LORA FOR DIFFUSION TRANSFORMERS
</title>


<link rel="shortcut icon" href="/icon.svg">


  <body>  
    <!-- 配置引用 -->
    <script>
  window.THEME_CONFIG = {
    image: {
      enable: 'true',
      center: 'true',
      caption: 'true'
    },
    code_block: {
      copy_button: "icon/copy.svg",
      theme_toggle: {
        enable: 'true',
        to_light_button: "icon/light.svg",
        to_dark_button: "icon/dark.svg",
        light_theme: "/css/vscode-modern-light.css",
        dark_theme: "/css/vscode-modern-dark.css"
      },
      mac_enhancer: {
        enable: 'true',
        init_folded_status: 'true'
      }
    },
    giscus: {
      repo: "Kytolly/MyGiscus",
      repo_id: "R_kgDOK_0_0A",
      category: "Announcements",
      category_id: "DIC_kwDOK_0_0A",
      mapping: "pathname",
      strict: "0",
      reactions_enabled: "1",
      emit_metadata: "0",
      input_position: "top",
      lang: "zh-CN",
      loading: "lazy"
    },
    heading_numbering: 'true',
    three_line_table: 'true',
    background: {
      enable: 'true',
      folder: 'img/',
      opacity: '0.1',
      position: 'center',
      size: 'cover',
      repeat: 'no-repeat',
      image: {
        mode: 'random',
        name: 'background.jpg',
        count: '113'
      }
    },
  };
</script>
<script>
  console.log(window.THEME_CONFIG);
</script>

<head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    
    <!-- 字体 -->
    <!-- 字体 -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@200..900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">

    <!-- 代码块 -->
    <link rel="preconnect" href="https://cdnjs.cloudflare.com">
    
      
        <link rel="stylesheet" href="/css/vscode-modern-light.css">
        <link rel="stylesheet" href="/css/vscode-modern-dark.css">
        <!-- DEBUG: url_for light path: /css/vscode-modern-light.css -->
        <!-- DEBUG: url_for dark path: /css/vscode-modern-dark.css -->
      
    
    <script src="https://unpkg.com/@highlightjs/cdn-assets@11.9.0/highlight.min.js"></script>

    <!-- 数学公式 -->
    
<link rel="stylesheet" href="/css/mathjax.css">

    
<script src="/js/mathjax.js"></script>

    
    <!-- 标题编号 -->
    
      
<link rel="stylesheet" href="/css/heading-numbering.css">

      
<script src="/js/heading-numbering.js"></script>

    
    
    <!-- 三线表样式 -->
    
      
<link rel="stylesheet" href="/css/three-line-table.css">

    
  
    <!-- 图片样式 -->
    
      
<link rel="stylesheet" href="/css/image-handler.css">

      
<script src="/js/image-handler.js"></script>

    
    
  
    
<link rel="stylesheet" href="/css/root.css">

    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/css/post.css">


    <!-- 代码块基础样式 -->
    
<link rel="stylesheet" href="/css/code-block-base.css">

  
    <!-- Codeblock Theme Toggler CSS -->
    
    
<link rel="stylesheet" href="/css/codeblock-theme-toggle.css">

    
  
    <!-- Codeblock Mac Enhancer CSS -->
    
    
<link rel="stylesheet" href="/css/code-block-mac-enhancer.css">

    
  
    <!-- TOC 样式 -->
    <link rel="stylesheet" href="/css/toc.css">
    
  <!-- hexo injector head_end start -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
  <!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0">
</head>


    <div class="site-container">
      <!-- 导航栏 -->
      <!--(头部 top bar) Header 样式 -->

<link rel="stylesheet" href="/css/header.css">


<header class="header">
  <section class="header-container">
    <a class="header-logo" href="/">Kytolly&#39;s Blog</a>
    <div class="header-right-section">
      <ul class="header-nav">
        
        <li>
          <a href="/">
            <img src="/icon/index.svg" alt="index icon" style="width:1.2em;height:1.2em;vertical-align:middle;">
            index
          </a>
        </li>
        
        <li>
          <a href="/archive">
            <img src="/icon/archive.svg" alt="archive icon" style="width:1.2em;height:1.2em;vertical-align:middle;">
            archive
          </a>
        </li>
        
        <li>
          <a href="/tag">
            <img src="/icon/tag.svg" alt="tag icon" style="width:1.2em;height:1.2em;vertical-align:middle;">
            tag
          </a>
        </li>
        
        <li>
          <a href="/about">
            <img src="/icon/about.svg" alt="about icon" style="width:1.2em;height:1.2em;vertical-align:middle;">
            about
          </a>
        </li>
        
      </ul>
      
      
      <div class="header-search-container search-trigger">
        <img src="/icon/search.svg" alt="Search Icon" class="search-icon" style="width:1.2em;height:1.2em;vertical-align:middle;">
        <input type="text" id="header-search-input" placeholder="searching..." readonly>
      </div>
      
    </div>
  </section>
</header>


<script src="/js/search.js"></script>


      <!-- 路由 根据页面类型决定渲染内容 -->
      <main class="main">
  
    <article class="post-container">
  <!-- 可选：左侧跟随目录 -->
  
  <aside id="toc-container" class="toc-container toc-left toc-expanded">
    <div id="toc-title" class="toc-title">IN-CONTEXT LORA FOR DIFFUSION TRANSFORMERS</div>
    <div id="toc-content" class="toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-Work"><span class="toc-number">2.</span> <span class="toc-text">Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Task-Specific-Image-Generation"><span class="toc-number">2.1.</span> <span class="toc-text">Task-Specific Image Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Task-Agnostic-Image-Generation"><span class="toc-number">2.2.</span> <span class="toc-text">Task-Agnostic Image Generation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Method"><span class="toc-number">3.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Problem-Formulation"><span class="toc-number">3.1.</span> <span class="toc-text">Problem Formulation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Group-Diffusion-Transformers"><span class="toc-number">3.2.</span> <span class="toc-text">Group Diffusion Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#In-Context-LoRA"><span class="toc-number">3.3.</span> <span class="toc-text">In-Context LoRA</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiments"><span class="toc-number">4.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-Details"><span class="toc-number">4.1.</span> <span class="toc-text">Implementation Details</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results"><span class="toc-number">4.2.</span> <span class="toc-text">Results</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Reference-Free-Image-Set-Generation"><span class="toc-number">4.2.1.</span> <span class="toc-text">Reference-Free Image-Set Generation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reference-Based-Image-Set-Generation"><span class="toc-number">4.2.2.</span> <span class="toc-text">Reference-Based Image-Set Generation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reproducing"><span class="toc-number">5.</span> <span class="toc-text">Reproducing</span></a></li></ol>
    </div>
  </aside>
 


<script src="/js/toc.js"></script>


  <!-- 可选：右侧 AI 摘要 -->
  

  <div class="post-main-wrapper">
    <!-- 文章 -->
    <div class="post-title">IN-CONTEXT LORA FOR DIFFUSION TRANSFORMERS</div>
  <div class="post-meta">
    <div class="date">2025 十月 6日</div>
    <div class="tags">
      
        
        <div class="tag-item">papers</div>
        
        <div class="tag-item">Computer-Vision</div>
        
      
    </div>
  </div>

<main class="post-content"><p><strong>Abstract</strong></p>
<p>最近的研究探索了使用 diffusion transformers（DiT）通过简单地在图像之间 concatenating attention tokens 来生成 task-agnostic 图像。然而，尽管有大量的计算资源，生成的图像的保真度仍然不理想。在这项研究中，我们通过假设 <strong>text-to-image 的 DiT 固有地拥有 in-context generation 的能力</strong>，只需最低限度的调整即可激活它们，从而重新评估和简化了这个框架。通过不同的任务实验，我们定性地证明了现有的 text-to-image DiT可以在无需任何调整的情况下有效地执行上下文生成。基于这一见解，我们提出了一个非常简单的pipeline 来利用 DiT 的上下文能力：</p>
<ol>
<li>连接图像而不是 tokens;</li>
<li>执行多个图像的 joint captioning;</li>
<li>使用小数据集应用特定于任务的LoRA调整（例如，20 ~ 100个样本），而不是对大型数据集进行全参数调优。</li>
</ol>
<p>我们将我们的模型命名为 In-Context LoRA（IC-LoRA）。这种方法不需要修改原始 DiT 模型，只需要更改训练数据。值得注意的是，我们的 pipeline 可以生成更好地遵守提示的高保真图像集。虽然在调优数据方面是特定于任务的，但我们的框架在架构和管道方面仍然保持 task-agnostic，这为社区提供了强大的工具，并为进一步研究产品级任务不可知生成系统提供了有价值的见解。我们在 <a target="_blank" rel="noopener" href="https://github.com/ali-vilab/In-Context-LoRA">GitHub</a>上发布我们的代码、数据和模型。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2>
<p>文本到图像模型的出现显着推进了视觉内容生成领域，使能够从文本描述创建高保真图像。现在，许多方法都提供了对各种图像属性的增强控制，允许在生成期间进行更细的调整。尽管取得了这些进展，适应文本到图像的模型，以广泛的生成任务，特别是那些需要连贯的图像集与复杂的内在关系，仍然是一个开放的挑战。在这项工作中，我们介绍了一个 task-agnostic 的框架，<strong>旨在适应文本到图像模型，以不同的生成任务，旨在提供一个通用的解决方案，多功能和可控的图像生成</strong>。</p>
<blockquote>
<p>传统方法（任务特定）：要做图像分类，就训练一个分类网络（如ResNet）。要做目标检测，就训练一个检测网络（如Faster R-CNN）。要做语义分割，就训练一个分割网络（如U-Net）;每个模型都是“各自为政”，从零开始或在特定数据集上微调，学到的特征只擅长自己的任务。</p>
<p>任务无关的方法：先使用一种通用的、自监督的预训练方法（如MAE, SimCLR, DINO）在一个巨大的、无标签的图像库（如ImageNet）上训练一个模型。这个模型学习到的<strong>图像特征</strong>就是“任务无关”的。它不知道也不关心你将来要用它来分类、检测还是分割。它只是学会了如何很好地“理解”图像的内容和结构。当需要解决具体任务时，只需在这个通用的“任务无关”模型后面添加一个非常简单的<strong>任务特定头</strong>（比如一个分类层），然后用少量数据进行微调，甚至不微调（线性探测），就能取得非常好的效果。</p>
<p>任务无关的框架是一个系统或模型架构，它被设计成能够<strong>不经过核心结构修改</strong>，直接处理多种不同的视觉任务。这个框架的核心是一个共享的、任务无关的骨干网络，它负责从图像中提取通用的特征。然后，通过<strong>附加不同的、轻量级的“任务头”或使用不同的提示指令</strong>，来引导这个骨干网络解决不同的问题。</p>
</blockquote>
<p>最近的工作，例如 Group Diffusion Transformers（GDT）框架，探索了将视觉生成任务重新定义为群体生成问题。在这种情况下，具有任意 intrinsic relationships 的一组图像在单个去噪扩散过程中同时生成，可选地以另一组图像为条件。GDT 的核心思想是将图像中的注意力标记连接起来，包括条件标记和要生成的标记，同时确保每个图像的标记只关注其相应的文本标记。这种方法允许模型以与任务无关的零触发方式适应多个任务，而无需任何微调或梯度更新。</p>
<p>然而，尽管 GDT 具有创新的架构，但其generation fidelity 相对较低，与原始预训练的文本到图像模型相比，通常表现不佳。这一限制促使人们重新审视将文本到图像模型适应复杂生成任务时所采用的基本假设和方法。</p>
<p>在这项工作中，我们做出了一个关键假设：<strong>text-to-image 模型固有地拥有in-context generation capabilities</strong> 。为了验证这一点，我们直接将现有的 text-to-image 模型应用于各种需要生成具有不同关系的图像集的任务。如图3所示；</p>
<p><em>FLUX文本到图像生成示例。使用 FLUX.1-dev 跨六个任务生成文本到图像的示例，突出显示了具有不同关系属性的多面板图像的创建。主要观察结果包括：（1）原始的文本到图像模型已经可以生成在身份、风格、照明和字体方面具有连贯一致性的多面板输出，尽管仍然存在一些小的缺陷。(2)FLUX.1-dev在解释描述多个面板的组合提示方面表现出强大的能力。</em></p>
<p><img src="assets/papers/IN-CONTEXT-LORA-FOR-DIFFUSION-TRANSFORMERS/FLUX-Text-to-Image-Generation-Examples.png" alt="FLUX Text-to-Image Generation Examples."></p>
<p>以 FLUX.1-dev 模型为例，我们观察到该模型已经可以执行不同的任务，尽管存在一些缺陷。它保持一致的属性，例如对象身份、风格、照明条件和调色板，同时修改姿势、3D方向和布局等其他方面。此外，该模型还展示了在单个合并提示内解释和遵循多个图像描述的能力。</p>
<p>这些令人惊讶的发现让我们得到了几个关键见解：</p>
<ol>
<li><strong>固有上下文学习</strong>：文本到图像模型已经具备上下文生成能力。通过适当触发和增强这种能力，我们可以利用它来执行复杂的生成任务。</li>
<li><strong>无需架构修改的模型可重用性</strong>：由于 text-to-image 模型可以解释合并的captions，因此我们可以重用它们在上下文中生成，而无需对其架构进行任何更改。这涉及简单地更改输入数据，而不是修改模型本身。</li>
<li><strong>最少数据和计算的效率</strong>：无需大型数据集或延长训练时间即可实现高质量的结果。小型、高质量的数据集加上最少的计算资源可能就足够了。</li>
</ol>
<p>基于这些见解，我们设计了一个极其简单但有效的 pipeline，用于使 text-to-image 模型适应不同的任务。我们的方法在以下方面与 GDT 形成鲜明对比：</p>
<ol>
<li><strong>图像拼接</strong>：我们将一组图像拼接成单个大图像，而不是拼接注意力标记。该方法大致相当于 diffusion transformers（DiT）中的 token concatenation，忽略了 Variational Autoencoder（VAE）组件引入的差异。</li>
<li><strong>prompts 拼接</strong>：我们将每个图像的 prompts 合并为一个长prompt，使模型能够同时处理和生成多个图像。这与 GDT 方法不同，在GDT方法中，每个图像的 tokens 都与其文本 tokens 交叉。</li>
<li><strong>使用小数据集进行最小微调</strong>：我们没有对数十万个样本进行大规模训练，而是使用一小组仅 <svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="8.673ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 3833.6 688" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-223C" d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use><use data-c="30" xlink:href="#MJX-1-TEX-N-30" transform="translate(500,0)"></use></g><g data-mml-node="mo" transform="translate(1277.8,0)"><use data-c="223C" xlink:href="#MJX-1-TEX-N-223C"></use></g><g data-mml-node="mn" transform="translate(2333.6,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use><use data-c="30" xlink:href="#MJX-1-TEX-N-30" transform="translate(500,0)"></use><use data-c="30" xlink:href="#MJX-1-TEX-N-30" transform="translate(1000,0)"></use></g></g></g></svg>个图像集来微调模型的 Low-Rank Adaptation（LoRA）。这种方法显着减少了所需的计算资源，并在很大程度上保留了原始文本到图像模型的知识和上下文能力。</li>
</ol>
<p>生成的模型非常简单，不需要修改原始的 text-to-image 模型。调整仅通过根据特定任务需求调整一小组调整数据来实现。为了支持image-conditional generation，我们采用了一种简单的技术：在连接的大图像中 mask 一个或多个图像，并 prompt 模型使用剩余图像修补它们。为此，我们直接利用SDEDit。</p>
<p>尽管它的简单性，我们发现，我们的方法可以适应各种各样的高质量的任务。虽然我们的方法需要特定于任务的调优数据，但整体框架和 pipeline 仍然与任务无关，允许在不修改原始模型架构的情况下适应各种任务。这种最低数据要求和广泛适用性的结合为生成社区，设计师和艺术家提供了强大的工具。我们承认，开发一个完全统一的发电系统仍然是一个开放的挑战，并将其作为未来的工作。为了便于进一步研究，我们在项目页面2发布了我们的数据、模型和训练配置。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2>
<h3 id="Task-Specific-Image-Generation"><a href="#Task-Specific-Image-Generation" class="headerlink" title="Task-Specific Image Generation"></a>Task-Specific Image Generation</h3>
<p>Text-to-image 模型在从复杂的文本提示生成高保真图像方面取得了显着的成功。然而，它们通常缺乏对生成图像的特定属性的细粒度可控性。为了解决这一限制，人们提出了许多工作来增强对布局等方面的控制和照明条件。有些方法甚至支持同时生成多个图像，类似于我们的方法。</p>
<p>尽管取得了这些进步，但这些模型通常采用特定于任务的架构和 pipelines，限制了它们的灵活性和通用性。每个架构都是针对单个任务量身定制的，并且为一项任务开发的功能不容易组合或扩展到任意新任务。这与自然语言处理的最近进展形成鲜明对比，其中模型被设计为在单个体系结构中执行多个任务，并且可以概括超出它们显式训练的任务。</p>
<h3 id="Task-Agnostic-Image-Generation"><a href="#Task-Agnostic-Image-Generation" class="headerlink" title="Task-Agnostic Image Generation"></a>Task-Agnostic Image Generation</h3>
<p>为了克服任务特定模型的限制，最近的研究旨在创建任务不可知的框架，该框架支持单个架构内的多个可控图像生成任务。例如，Emu Edit 集成了广泛的图像编辑功能，而Emu 2，TransFusion ，和 OmniGen 在一个统一的模型中执行不同的任务，从 procedural drawing 到 subject-driven generation。Emu 3通过在一个框架下支持文本、图像和视频生成进一步扩展了这一功能。这些工作代表了统一或任务不可知的一代的实质性进展。</p>
<p>与这些模型相反，我们提出现有的 text-to-image 架构已经具备内在的上下文能力。这消除了开发新架构的需要，并以最少的额外数据和计算资源实现高质量生成。我们的方法不仅提高了效率，而且还在各种任务中提供卓越的发电质量。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2>
<h3 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h3>
<p>遵循 Group Diffusion Transformers 的方法[Huang等人，2024]，我们将大多数图像生成任务定义为产生一组 <svg style="vertical-align: -0.312ex;" xmlns="http://www.w3.org/2000/svg" width="5.506ex" height="1.819ex" role="img" focusable="false" viewBox="0 -666 2433.6 804" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-2265" d="M83 616Q83 624 89 630T99 636Q107 636 253 568T543 431T687 361Q694 356 694 346T687 331Q685 329 395 192L107 56H101Q83 58 83 76Q83 77 83 79Q82 86 98 95Q117 105 248 167Q326 204 378 228L626 346L360 472Q291 505 200 548Q112 589 98 597T83 616ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-2-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(877.8,0)"><use data-c="2265" xlink:href="#MJX-2-TEX-N-2265"></use></g><g data-mml-node="mn" transform="translate(1933.6,0)"><use data-c="31" xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></svg>图像，条件是另一组<svg style="vertical-align: -0.312ex;" xmlns="http://www.w3.org/2000/svg" width="6.135ex" height="1.819ex" role="img" focusable="false" viewBox="0 -666 2711.6 804" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-2265" d="M83 616Q83 624 89 630T99 636Q107 636 253 568T543 431T687 361Q694 356 694 346T687 331Q685 329 395 192L107 56H101Q83 58 83 76Q83 77 83 79Q82 86 98 95Q117 105 248 167Q326 204 378 228L626 346L360 472Q291 505 200 548Q112 589 98 597T83 616ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path><path id="MJX-3-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D45A" xlink:href="#MJX-3-TEX-I-1D45A"></use></g><g data-mml-node="mo" transform="translate(1155.8,0)"><use data-c="2265" xlink:href="#MJX-3-TEX-N-2265"></use></g><g data-mml-node="mn" transform="translate(2211.6,0)"><use data-c="30" xlink:href="#MJX-3-TEX-N-30"></use></g></g></g></svg>图像和<svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.87ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3478.4 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-4-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-4-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-4-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-4-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-4-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><use data-c="28" xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389,0)"><use data-c="1D45B" xlink:href="#MJX-4-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(1211.2,0)"><use data-c="2B" xlink:href="#MJX-4-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(2211.4,0)"><use data-c="1D45A" xlink:href="#MJX-4-TEX-I-1D45A"></use></g><g data-mml-node="mo" transform="translate(3089.4,0)"><use data-c="29" xlink:href="#MJX-4-TEX-N-29"></use></g></g></g></svg>个文本提示。该形式化涵盖广泛的学术任务，例如图像翻译、风格转移、姿势转移和主题驱动生成，以及图画书创作、字体设计和转移、故事板生成等实际应用。条件图像和生成的图像之间的相关性通过每个图像提示隐式地维护。</p>
<p>我们的方法通过对整个图像集使用一个consolidated prompt 来稍微修改这个框架。此 prompt 通常从图像集的总体描述开始，然后是每个图像的单独提示。这种统一的 prompt 设计与现有的 text-to-image 模型更兼容，并允许整体描述自然地传达任务的意图，就像客户如何向艺术家传达设计要求一样。</p>
<h3 id="Group-Diffusion-Transformers"><a href="#Group-Diffusion-Transformers" class="headerlink" title="Group Diffusion Transformers"></a>Group Diffusion Transformers</h3>
<p>我们从基本框架开始，Group Diffusion Transformers（GDT）。在GDT中，通过在每个 Transformer self-attention block 中的图像之间连接 attention tokens，在单个扩散过程中同时生成一组图像。这种方法使每个图像都能够“看到”集中的所有其他图像并与其交互。Text conditioning 是通过让每个图像注意其相应的 text embeddings 来引入的，使其能够访问其他图像的内容和相关的 text guidance。  GDT在数十万个图像集上训练，使其能够以零镜头方式在任务中进行概括。</p>
<h3 id="In-Context-LoRA"><a href="#In-Context-LoRA" class="headerlink" title="In-Context LoRA"></a>In-Context LoRA</h3>
<p>虽然 GDT 展示了零镜头任务适应性，但其生成质量不足，与text-to-image models模型的 baseline  相比，通常表现不佳。我们提出了一些改进措施来改进这个框架。</p>
<p>我们的出发点是假设基础的 text-to-image 模型固有地拥有一些针对不同任务的上下文生成能力，即使质量有所不同。下图中的结果支持了这一点，其中模型有效地生成跨不同任务的多个图像（有时有条件）。</p>
<p><em>FLUX文本到图像生成示例。使用 FLUX.1-dev 跨六个任务生成文本到图像的示例，突出显示了具有不同关系属性的多面板图像的创建。主要观察结果包括：（1）原始的文本到图像模型已经可以生成在身份、风格、照明和字体方面具有连贯一致性的多面板输出，尽管仍然存在一些小的缺陷。(2) FLUX.1-dev在解释描述多个面板的组合提示方面表现出强大的能力，详情请在附录A中。</em></p>
<p><img src="assets/papers/IN-CONTEXT-LORA-FOR-DIFFUSION-TRANSFORMERS/FLUX-Text-to-Image-Generation-Examples.png" alt="FLUX Text-to-Image Generation Examples."></p>
<p>基于这一见解，不需要对大型数据集进行广泛的训练;我们可以通过精心策划的高质量图像集激活模型的上下文能力。</p>
<p>另一个观察结果是，text-to-image 模型可以从包含多个面板描述的单个 prompt 生成连贯的 multi-panel 图像。</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(a) Portrait Photography. This four-panel image captures a young boy’s adventure in the woods, expressing curiosity and wonder. [TOP-LEFT] He crouches beside a stream, peering intently at a group of frogs jumping along the rocks, his face full of excitement; [TOP-RIGHT] he climbs a low tree branch, arms stretched wide as he balances, a big grin on his face; [BOTTOM-LEFT] a close-up shows him kneeling in the dirt, inspecting a bright yellow mushroom with fascination; [BOTTOM-RIGHT] the boy runs through a clearing, his arms spread out like airplane wings, lost in the thrill of discovery.  </span><br><span class="line"></span><br><span class="line">(b) Product Design. The image showcases a modern and multifunctional baby walker designed for play and growth, featuring its versatility and attention to detail; [TOP-LEFT] the first panel highlights the walker’s sleek form with several interactive toys on the tray, focusing on its overall structure and functionality, [TOP-RIGHT] the second panel provides a side view emphasizing the built-in lighting around the play tray, illustrating both style and safety features, [BOTTOM-LEFT] the third panel displays a rear view of the walker showcasing the comfortable seat and adjustable design elements, underlining comfort and adaptability, [BOTTOM-RIGHT] while the final panel offers a close-up of the activity center with various colorful toys, capturing its playful appeal and engagement potential.  </span><br><span class="line"></span><br><span class="line">(c) Font Design. The four-panel image emphasizes the versatility of a minimalist sans-serif font across various elegant settings: [TOP-LEFT] displays the word “Essence” in muted beige, featured on a luxury perfume bottle with a marble backdrop; [TOP-RIGHT] shows the phrase “Pure Serenity” in soft white, set against an image of serene, rippling water; [BOTTOM-LEFT] showcases “Breathe Deep” in pale blue, printed on a calming lavender candle, evoking a spa-like atmosphere; [BOTTOM-RIGHT] features “Elegance Defined” in charcoal gray, embossed on a sleek hardcover notebook, emphasizing sophistication and style.  </span><br><span class="line"></span><br><span class="line">(d) Sandstorm Visual Effect. The two-panel image showcases a biker speeding through a desert landscape before and after a sandstorm effect, capturing a powerful transformation; [TOP] the first panel presents a biker riding along a dirt path, with the vast desert and blue sky stretching out behind them, conveying a sense of freedom and adventure, while [BOTTOM] the second panel introduces a violent sandstorm, with grains of sand swirling around the biker and partially obscuring the landscape, transforming the scene into a chaotic and thrilling visual spectacle.  </span><br><span class="line"></span><br><span class="line">(e) Visual Identity Design. This two-panel image captures the essence of a visual identity design and its adaptable application, showcasing both the original concept and its practical derivative use; [LEFT] the left panel presents a bright and engaging graphic featuring a stylized gray koala character triumphantly holding a large wedge of cheese on a vibrant yellow background, using bold black outlines to emphasize the simplicity and playfulness of the design, while [RIGHT] the right panel illustrates the design’s extension to everyday objects, where the same koala and cheese motif has been skillfully adapted onto a circular coaster with a softer yellow tone, accompanied by a matching mug bearing smaller graphics of the koala and cheese, both items resting elegantly on a minimalist white table, highlighting the versatility and cohesive appeal of the visual identity across different mediums. </span><br><span class="line"></span><br><span class="line">(f) Portrait Illustration. This two-panel image showcases a transformation from a photographic portrait to a playful illustration; [LEFT] the first panel displays a man in a navy suit, white shirt, and brown shoes, sitting on a wooden bench in an urban park, his hand resting casually on his lap; [RIGHT] the illustration panel transforms him into a cartoon-like character, with smooth lines and exaggerated features, including oversized shoes and a vibrant blue suit, set against a minimalist park backdrop, giving the scene a lively and humorous feel.</span><br></pre></td></tr></table></figure>
<p>因此，我们可以通过使用统一的图像提示来简化架构，而不是要求每个图像专门关注其各自的文本标记。这使我们能够重复使用原始的 text-to-image架构，而无需进行任何结构上的修改。</p>
<p>我们最终的框架设计通过在训练期间将一组图像直接连接成一个大图像，同时生成一组图像，同时将它们的 captions 合并成一个合并的 prompt，并为每个面板提供总体描述和明确的指导。生成图像集后，我们将大图像拆分为各个面板。此外，由于 text-to-image 模型已经展示了上下文能力，因此我们不会对整个模型进行微调。相反，我们对一小群高质量数据应用Low-Rank Adaptation（LoRA）来触发和增强这些功能。</p>
<blockquote>
<p>Low-Rank Adaptation(LoRA, 大模型的低秩自适应) 是一种用于<strong>高效微调</strong>大型预训练模型（如GPT、扩散模型、BERT等）的技术。</p>
<ul>
<li>传统微调：好比为了给一件西装换个款式，你把整件衣服拆了，重新裁剪缝合所有布料。这需要很高的手艺（计算资源）和很长时间，而且容易把原来的好布料（预训练中获得的知识）弄坏。</li>
<li>LoRA微调：好比在这件西装的关键部位（比如肩部、腰部）用别针固定上几块小小的、新的布料。这些小块布料专门用于调整款式，非常轻便，而且随时可以取下，恢复西装原样。</li>
</ul>
<p>LoRA基于以下假设：模型在适应新任务时，其权重变化<svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.256ex" height="1.67ex" role="img" focusable="false" viewBox="0 -716 1881 738" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-5-TEX-I-1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path><path id="MJX-5-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D6E5" xlink:href="#MJX-5-TEX-I-1D6E5"></use></g><g data-mml-node="mi" transform="translate(833,0)"><use data-c="1D44A" xlink:href="#MJX-5-TEX-I-1D44A"></use></g></g></g></svg>具有“低秩”特性。这意味着不需要一个完整的、高维度的矩阵来表示所有的变化，而是可以用两个<strong>更小、更薄、可训练的矩阵的乘积</strong>作为<strong>旁路</strong>近似这个变化。在微调过程中，<strong>只更新A和B这两个小矩阵的参数</strong>，原始权重<svg style="vertical-align: -0.038ex;" xmlns="http://www.w3.org/2000/svg" width="2.731ex" height="1.59ex" role="img" focusable="false" viewBox="0 -686 1207 703" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-6-TEX-BI-1D47E" d="M111 624Q109 624 102 624T91 623Q61 623 61 640Q61 660 70 678Q78 686 98 686Q140 684 239 684Q277 684 309 684T360 685T383 686H385Q407 686 407 668Q404 634 391 626Q387 624 348 624Q307 624 307 622Q307 618 332 409Q359 198 359 195L570 532L564 576L558 622V624H522H504Q472 624 472 641Q475 678 488 684L493 686L529 685Q551 684 645 684Q716 684 753 685T795 686Q818 686 818 669Q815 632 802 626Q798 624 759 624Q718 624 718 622Q718 615 743 410Q770 199 770 196Q770 195 806 253T903 406Q1035 618 1035 619Q1025 624 968 624Q943 624 943 641Q943 648 946 659Q950 675 952 679T963 686L998 685Q1020 684 1093 684Q1113 684 1139 685T1173 686Q1207 686 1207 669Q1207 664 1204 652Q1199 631 1194 628T1164 624Q1113 622 1101 615Q1098 612 905 305Q715 -1 709 -7Q699 -17 673 -17Q645 -17 639 -8L581 441Q581 444 442 221Q331 44 314 18T288 -14Q279 -17 263 -17H254Q229 -17 227 -5Q225 2 186 311L147 620V624H111Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D47E" xlink:href="#MJX-6-TEX-BI-1D47E"></use></g></g></g></svg>保持不变。矩阵A和B的秩非常小，这使得它们包含的参数数量极少</p>
<ul>
<li>矩阵<svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.966ex" height="1.609ex" role="img" focusable="false" viewBox="0 -711 869 711" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-7-TEX-BI-1D468" d="M65 0Q45 0 45 18Q48 52 61 60Q65 62 81 62Q155 62 165 74Q166 74 265 228T465 539T569 699Q576 707 583 709T611 711T637 710T649 700Q650 697 695 380L741 63L784 62H827Q839 50 839 45L835 29Q831 9 827 5T806 0Q803 0 790 0T743 1T657 2Q585 2 547 1T504 0Q481 0 481 17Q484 54 497 60Q501 62 541 62Q580 62 580 63Q580 68 573 121T564 179V181H308L271 124Q236 69 236 67T283 62H287Q316 62 316 46Q316 26 307 8Q302 3 295 0L262 1Q242 2 168 2Q119 2 93 1T65 0ZM537 372Q533 402 528 435T521 486T518 504V505Q517 505 433 375L348 244L451 243Q555 243 555 244L537 372Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D468" xlink:href="#MJX-7-TEX-BI-1D468"></use></g></g></g></svg>负责将输入数据<svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-8-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-8-TEX-I-1D465"></use></g></g></g></svg>降维;</li>
<li>矩阵<svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.552ex" role="img" focusable="false" viewBox="0 -686 866 686" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-9-TEX-BI-1D469" d="M258 624H235Q214 624 209 626T199 639Q203 678 216 684Q220 686 449 686H477H586Q684 686 733 677T817 634Q853 598 853 547Q853 499 826 460T761 401T695 371T654 360H653L662 358Q670 357 683 354T712 344T744 327T774 303T795 269T804 224Q804 148 732 79T533 1Q524 0 288 0H58Q47 5 43 15Q47 54 60 60Q64 62 113 62H162L302 623Q302 624 258 624ZM703 550Q703 571 695 586T675 609T656 619T643 623L545 624H447L417 504Q386 384 386 383T470 382Q554 383 565 385Q632 397 667 447T703 550ZM651 240Q651 265 645 282T626 309T608 322T592 329Q587 330 479 331H373L340 198Q307 65 306 64Q306 62 406 62L507 63L519 65Q565 76 596 107T639 171T651 240Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D469" xlink:href="#MJX-9-TEX-BI-1D469"></use></g></g></g></svg>负责将降维后的数据再升维回原始维度;</li>
<li>原始模型<svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="8.345ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 3688.6 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-10-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path id="MJX-10-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-10-TEX-BI-1D47E" d="M111 624Q109 624 102 624T91 623Q61 623 61 640Q61 660 70 678Q78 686 98 686Q140 684 239 684Q277 684 309 684T360 685T383 686H385Q407 686 407 668Q404 634 391 626Q387 624 348 624Q307 624 307 622Q307 618 332 409Q359 198 359 195L570 532L564 576L558 622V624H522H504Q472 624 472 641Q475 678 488 684L493 686L529 685Q551 684 645 684Q716 684 753 685T795 686Q818 686 818 669Q815 632 802 626Q798 624 759 624Q718 624 718 622Q718 615 743 410Q770 199 770 196Q770 195 806 253T903 406Q1035 618 1035 619Q1025 624 968 624Q943 624 943 641Q943 648 946 659Q950 675 952 679T963 686L998 685Q1020 684 1093 684Q1113 684 1139 685T1173 686Q1207 686 1207 669Q1207 664 1204 652Q1199 631 1194 628T1164 624Q1113 622 1101 615Q1098 612 905 305Q715 -1 709 -7Q699 -17 673 -17Q645 -17 639 -8L581 441Q581 444 442 221Q331 44 314 18T288 -14Q279 -17 263 -17H254Q229 -17 227 -5Q225 2 186 311L147 620V624H111Z"></path><path id="MJX-10-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-10-TEX-I-210E"></use></g><g data-mml-node="mo" transform="translate(853.8,0)"><use data-c="3D" xlink:href="#MJX-10-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1909.6,0)"><use data-c="1D47E" xlink:href="#MJX-10-TEX-BI-1D47E"></use></g><g data-mml-node="mi" transform="translate(3116.6,0)"><use data-c="1D465" xlink:href="#MJX-10-TEX-I-1D465"></use></g></g></g></svg>微调为<svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="16.33ex" height="1.794ex" role="img" focusable="false" viewBox="0 -711 7218 793" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-11-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path id="MJX-11-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-11-TEX-BI-1D47E" d="M111 624Q109 624 102 624T91 623Q61 623 61 640Q61 660 70 678Q78 686 98 686Q140 684 239 684Q277 684 309 684T360 685T383 686H385Q407 686 407 668Q404 634 391 626Q387 624 348 624Q307 624 307 622Q307 618 332 409Q359 198 359 195L570 532L564 576L558 622V624H522H504Q472 624 472 641Q475 678 488 684L493 686L529 685Q551 684 645 684Q716 684 753 685T795 686Q818 686 818 669Q815 632 802 626Q798 624 759 624Q718 624 718 622Q718 615 743 410Q770 199 770 196Q770 195 806 253T903 406Q1035 618 1035 619Q1025 624 968 624Q943 624 943 641Q943 648 946 659Q950 675 952 679T963 686L998 685Q1020 684 1093 684Q1113 684 1139 685T1173 686Q1207 686 1207 669Q1207 664 1204 652Q1199 631 1194 628T1164 624Q1113 622 1101 615Q1098 612 905 305Q715 -1 709 -7Q699 -17 673 -17Q645 -17 639 -8L581 441Q581 444 442 221Q331 44 314 18T288 -14Q279 -17 263 -17H254Q229 -17 227 -5Q225 2 186 311L147 620V624H111Z"></path><path id="MJX-11-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-11-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-11-TEX-BI-1D469" d="M258 624H235Q214 624 209 626T199 639Q203 678 216 684Q220 686 449 686H477H586Q684 686 733 677T817 634Q853 598 853 547Q853 499 826 460T761 401T695 371T654 360H653L662 358Q670 357 683 354T712 344T744 327T774 303T795 269T804 224Q804 148 732 79T533 1Q524 0 288 0H58Q47 5 43 15Q47 54 60 60Q64 62 113 62H162L302 623Q302 624 258 624ZM703 550Q703 571 695 586T675 609T656 619T643 623L545 624H447L417 504Q386 384 386 383T470 382Q554 383 565 385Q632 397 667 447T703 550ZM651 240Q651 265 645 282T626 309T608 322T592 329Q587 330 479 331H373L340 198Q307 65 306 64Q306 62 406 62L507 63L519 65Q565 76 596 107T639 171T651 240Z"></path><path id="MJX-11-TEX-BI-1D468" d="M65 0Q45 0 45 18Q48 52 61 60Q65 62 81 62Q155 62 165 74Q166 74 265 228T465 539T569 699Q576 707 583 709T611 711T637 710T649 700Q650 697 695 380L741 63L784 62H827Q839 50 839 45L835 29Q831 9 827 5T806 0Q803 0 790 0T743 1T657 2Q585 2 547 1T504 0Q481 0 481 17Q484 54 497 60Q501 62 541 62Q580 62 580 63Q580 68 573 121T564 179V181H308L271 124Q236 69 236 67T283 62H287Q316 62 316 46Q316 26 307 8Q302 3 295 0L262 1Q242 2 168 2Q119 2 93 1T65 0ZM537 372Q533 402 528 435T521 486T518 504V505Q517 505 433 375L348 244L451 243Q555 243 555 244L537 372Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-11-TEX-I-210E"></use></g><g data-mml-node="mo" transform="translate(853.8,0)"><use data-c="3D" xlink:href="#MJX-11-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(1909.6,0)"><use data-c="1D47E" xlink:href="#MJX-11-TEX-BI-1D47E"></use></g><g data-mml-node="mi" transform="translate(3116.6,0)"><use data-c="1D465" xlink:href="#MJX-11-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(3910.8,0)"><use data-c="2B" xlink:href="#MJX-11-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(4911,0)"><use data-c="1D469" xlink:href="#MJX-11-TEX-BI-1D469"></use></g><g data-mml-node="mi" transform="translate(5777,0)"><use data-c="1D468" xlink:href="#MJX-11-TEX-BI-1D468"></use></g><g data-mml-node="mi" transform="translate(6646,0)"><use data-c="1D465" xlink:href="#MJX-11-TEX-I-1D465"></use></g></g></g></svg>;</li>
</ul>
</blockquote>
<p>为了支持对额外图像集的条件处理，我们使用 SDEDit（一种免训练方法）来基于 unmasked set 来修补一组图像，所有图像都连接在单个大图像中。</p>
<blockquote>
<p>Stochastic Differential Editing(SDEdit, 随机微分编辑) 是一种Diffusion Model-based、train-free的图像生成与编辑方法。可以理解为一种<strong>模糊重绘技术</strong>：先给输入图像添加噪声，使其变模糊，然后利用 Diffusion Model，在降噪过程中重新绘制出一张既符合你输入的结构、又逼真自然的新图像。</p>
</blockquote>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2>
<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3>
<p>我们在 FLUX.1-dev  text-to-image 模型上构建我们的方法，并专门针对我们的任务训练 In-Context LoRA。我们选择一系列实用的任务，包括storyboard generation、font design、portrait photography、visual identity design、home decoration、visual effects、portrait illustration和 PowerPoint template design等。对于每个任务，我们从互联网上收集 20 到 100 个高质量图像集。每一组都连接成一张合成图像，并使用 Multi-modal Large Language Models (MLLMs) 生成这些图像的字幕，首先是总体摘要，然后是每个图像的详细描述。训练在<strong>单个A100 GPU</strong>上进行，执行 5000 个步骤，批量大小为4，LoRA秩为 16。为了进行推断，我们采用了 20 个抽样步骤，指导标度为3.5，与 FLUX.1-dev 的蒸馏指导标度相匹配。对于图像条件生成，SDEDdit应用于旨在生成的 mask images，从而能够基于周围图像进行修补。</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3>
<p>我们提供定性结果，证明我们的模型在各种任务中的通用性和质量。鉴于任务的多样性，我们推迟对未来的工作采用统一的，定量的 benchmark 和评估。</p>
<h4 id="Reference-Free-Image-Set-Generation"><a href="#Reference-Free-Image-Set-Generation" class="headerlink" title="Reference-Free Image-Set Generation"></a>Reference-Free Image-Set Generation</h4>
<p>在此设置中，图像集仅根据文本提示生成，无需额外的图像输入。我们的方法在一系列图像集生成任务中实现了高质量的结果。</p>
<h4 id="Reference-Based-Image-Set-Generation"><a href="#Reference-Based-Image-Set-Generation" class="headerlink" title="Reference-Based Image-Set Generation"></a>Reference-Based Image-Set Generation</h4>
<p>在此设置中，使用 text prompt 和输入图像集 具有至少一个参考图像 生成图像集。SDEDdit用于 mask 某些图像，从而启用基于其余图像的修复。图像条件生成的结果如图13所示，常见的失败案例如图14所示。</p>
<p><em>图像条件生成。在多个任务中使用In-上下文LoRA以及免训练SDEDit的图像条件生成示例。在某些情况下，例如沙尘暴视觉效果的应用案例，输入和输出图像之间可能会出现不一致，包括汽车驾驶员身份和着装的变化。解决这些不一致之处留给未来的工作。</em></p>
<p><img src="assets/papers/IN-CONTEXT-LORA-FOR-DIFFUSION-TRANSFORMERS/Image-Conditional-Generation.png" alt="Image-Conditional Generation"></p>
<p><em>图像条件生成的失败案例。使用带SDEDit的In-Context LoRA的肖像身份传输失败的示例。我们观察到 In-Context LoRA 的 SDEDdit 往往不稳定，通常无法保留身份。这可能源于 SDEDdit对输入到输出映射的单向依赖性与 In-Context LoRA 训练的双向性质之间的差异。解决这个问题留给未来的工作。</em></p>
<p><img src="assets/papers/IN-CONTEXT-LORA-FOR-DIFFUSION-TRANSFORMERS/Failure-Cases-of-Image-Conditional-Generation.png" alt="Failure-Cases-of-Image-Conditional-Generation"></p>
<p>尽管在多个任务中有效，但与文本条件生成相比，图像之间的视觉一致性有时较低。这种差异可能是由于SDEDit在掩蔽和未掩蔽图像之间的单向依赖性造成的，而纯文本生成允许图像之间的双向依赖性，从而实现条件和输出的相互调整。这表明了改进的潜力，例如结合可训练的修复方法，我们将其留给未来的探索。</p>
<h2 id="Reproducing"><a href="#Reproducing" class="headerlink" title="Reproducing"></a>Reproducing</h2>
<p>克隆 <a target="_blank" rel="noopener" href="https://github.com/ostris/ai-toolkit">ai-toolkit</a> 仓库和 <a target="_blank" rel="noopener" href="https://github.com/ali-vilab/In-Context-LoRA">In-Context-LoRA</a> 仓库;</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ostris/ai-toolkit.git /opt/liblibai-models/user-workspace2/model_zoo</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/ali-vilab/In-Context-LoRA.git /opt/liblibai-models/user-workspace2/model_zoo</span><br></pre></td></tr></table></figure>
<p>创建虚拟环境</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda create -n aitk_xqy python=3.11</span><br><span class="line">conda activate aitk_xqy</span><br><span class="line">pip3 install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126</span><br><span class="line">pip3 install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>在命令行下登陆 <a target="_blank" rel="noopener" href="https://huggingface.co/">Huggingface</a>;</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br></pre></td></tr></table></figure>
<p>移动配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp /opt/liblibai-models/user-workspace2/model_zoo/In-Context-LoRA/config/movie-shots.yml /opt/liblibai-models/user-workspace2/model_zoo/ai-toolkit/config/</span><br></pre></td></tr></table></figure>
<p>执行训练</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/liblibai-models/user-workspace2/model_zoo/ai-toolkit</span><br><span class="line">python run.py config/movie-shots.yml</span><br></pre></td></tr></table></figure>
<p>可以看到加载了配置文件进行训练;</p>
<p>或者使用 UI 进行训练的配置，注意</p>
<ul>
<li>
<p>在本地访问<code>http://localhost:8675</code>,在服务器上执行如下命令（这个命令是可以中断的，无需保持 UI 运行即可运行作业，它只需要启动/停止/监控作业。）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ui</span><br><span class="line">npm run build_and_start</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>修改配置文件，指定 FLUX 模型为本地已经下载好的模型，而不是从 Huggingface 拉取；</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">        <span class="attr">name_or_path:</span> <span class="string">&quot;/opt/liblibai-models/user-workspace2/model_zoo/FLUX.1-dev&quot;</span></span><br><span class="line">        <span class="attr">is_flux:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">quantize:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>(opt) 开始训练前，在 Huggingface 上获取 Access Tokens；</p>
</li>
</ul>
<p>训练的 UI 界面如下：</p>
<p><img src="assets/papers/IN-CONTEXT-LORA-FOR-DIFFUSION-TRANSFORMERS/%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83%E7%9A%84UI%E7%95%8C%E9%9D%A2.png" alt="开始训练的UI界面"></p>
</main>


<script src="/js/theme.js"></script>


<script src="/js/code-block-header.js"></script>


<script src="/js/code-block-copy.js"></script>


<script src="/js/code-block-theme-toggle.js"></script>


<script src="/js/code-block.js"></script>



<link rel="stylesheet" href="/css/article.css">

    
    <!-- giscus评论区 -->
    
  <div id="giscus_thread"></div>

  </div>
</article>
  
</main>

      <!-- 页脚 -->
      <footer class="footer">
  
  <span>Copyright © Kytolly(shiroi)</span>
  
  <div class="theme-switch-wrapper">
    <div class="theme-switch">
      <div class="theme-switch-item light-mode active">
        <img src="/icon/light.svg" class="icon">
      </div>
      <div class="theme-switch-item dark-mode">
        <img src="/icon/dark.svg" class="icon">
      </div>
    </div>
  </div>
</footer>


<link rel="stylesheet" href="/css/footer.css">


<script src="/js/theme-switcher.js"></script>


<script src="/js/giscus.js"></script>


      
  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <img src="/icon/search.svg" alt="Search Icon" class="search-icon" style="width:1.2em;height:1.2em;vertical-align:middle;">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <img src="/icon/search.svg" alt="Search Icon" style="width:5em;height:5em;">
        </div>
        <ul id="search-results"></ul>
      </div>
    </div>
  </div>

<link rel="stylesheet" href="/css/search.css">



    </div>
    
<script src="/js/background.js"></script>

  </body>

</html>